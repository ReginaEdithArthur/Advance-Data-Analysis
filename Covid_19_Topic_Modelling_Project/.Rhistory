DTBJ
#### Exploring Difference Data ####
plot(DTBJ)
### Stationarity Tests for Difference Data ###
adf.test(DTBJ)
pp.test(DTBJ)
kpss.test(DTBJ)
#### Model Specification ####
ggAcf(DTBJ) # --- checking the MA values
ggPacf(DTBJ) # --- checking the AR values
# ARIMA111
ARIMA111 = Arima(TBJ, order = c(1, 1, 1), seasonal = c(0, 0, 0))
ARIMA111
# ARIMA211
ARIMA211 = Arima(TBJ, order = c(2, 1, 1), seasonal = c(0, 0, 0))
ARIMA211
# ARIMA112
ARIMA112 = Arima(TBJ, order = c(1, 1, 2), seasonal = c(0, 0, 0))
ARIMA112
# ARIMA212
ARIMA212 = Arima(TBJ, order = c(2, 1, 2), seasonal = c(0, 0, 0))
ARIMA212
# ARIMA113
ARIMA113 = Arima(TBJ, order = c(1, 1, 3), seasonal = c(0, 0, 0))
ARIMA113
# ARIMA213
ARIMA213 = Arima(TBJ, order = c(2, 1, 3), seasonal = c(0, 0, 0))
ARIMA213
# ARIMA114
ARIMA114 = Arima(TBJ, order = c(1, 1, 4), seasonal = c(0, 0, 0))
ARIMA114
# ARIMA214
ARIMA214 = Arima(TBJ, order = c(2, 1, 4), seasonal = c(0, 0, 0))
ARIMA214
#### Coefficient Test for Arima Model ####
coeftest(ARIMA111)
#### Diagnostics (Checking for Residuals), autocorrelation using p-value ####
arima111_residuals = residuals(ARIMA111)
arima111_residuals
plot(arima111_residuals)
checkresiduals(ARIMA111)
ArchTest(arima111_residuals) ## Arch Test: to detect the presence of time-varying volatility clustering in a time series
### Forecasting using the ARIMA Model ###
forecast(ARIMA111, h = 12)
plot(forecast(ARIMA111, h = 12))
BJ = BJsales
BJ
view(BJ)
View(BJ)
library(tseries)
library(lmtest)
library(tseries)
library(tseries)
library(lmtest)
library(FinTS)
library(forecast)
library(datasets)
BJ = BJsales
#### Convert to time series data ####
TBJ = ts(BJ, frequency = 12, start = c(2000, 1))
TBJ
#### Exploring the Data ####
plot(TBJ)
#### ACF Plot ####
ggAcf(TBJ)
### Augmented Test ###
adf.test(TBJ)
### Phillips–Perron test ###
pp.test(TBJ)
### KPSS Test ###
kpss.test(TBJ)
#### Difference data
DTBJ = diff(TBJ, differences=1)
DTBJ
#### Exploring the Data ####
plot(DTBJ)
### Augmented Test ### 
adf.test(DTBJ)
### Phillips–Perron test ###
pp.test(DTBJ)
### KPSS Test ###
kpss.test(DTBJ)
ggAcf(DTBJ)
ggPacf(DTBJ)
Arima111= Arima(TBJ,order=c(1,1,1),seasonal= c(0,0,0))
Arima111
Arima211= Arima(TBJ,order=c(2,1,1),seasonal= c(0,0,0))
Arima211
Arima112= Arima(TBJ,order=c(1,1,2),seasonal= c(0,0,0))
Arima112
Arima212= Arima(TBJ,order=c(2,1,2),seasonal= c(0,0,0))
Arima212
Arima113= Arima(TBJ,order=c(1,1,3),seasonal= c(0,0,0))
Arima113
Arima213= Arima(TBJ,order=c(2,1,3),seasonal= c(0,0,0))
Arima213
Arima114= Arima(TBJ,order=c(1,1,4),seasonal= c(0,0,0))
Arima114
Arima214= Arima(TBJ,order=c(2,1,4),seasonal= c(0,0,0))
Arima214
# Coefficient Test for Arima Model ####
coeftest(Arima111)
residuals(Arima111)
plot(residuals(Arima111))
checkresiduals(Arima111)
ArchTest(residuals(Arima111))
forecast(Arima111, h=24)
plot(forecast(Arima111, h=24))
print(Arima111)
length(TBJ)
forecast(Arima111)
library(tseries)
library(tseries)
library(lmtest)
library(forecast)
library(FinTS)
data = read.csv("C:/Users/Administrator/Desktop/Time Series Forecasting/bitcoin_price", header = TRUE, sep = ",")
#### Convert to time series data ####
TBJ = ts(data, frequency = 24, start = c(2000, 1))
TBJ
# Convert 'Date' column to Date type
data$Date = as.Date(data$Date, format = "%Y-%m-%d")
data = read.csv("C:/Users/Administrator/Desktop/Time Series Forecasting/bitcoin_price", header = TRUE, sep = ",")
# Convert 'Date' column to Date type
data$Date = as.Date(data$Date, format = "%Y-%m-%d")
#### Convert to time series data ####
TBJ = ts(data, frequency = 365, start = c(2014, 9, 17))
TBJ
# Convert 'Date' column to Date type
data$Date = as.Date(data$Date, format = "%Y-%m-%d")
# Ensure it's sorted correctly
data <- data[order(data$Date), ]
#### Convert to time series data ####
TBJ = ts(data, frequency = 365, start = c(2014, 9, 17))
TBJ
#### Convert to time series data ####
TBJ = ts(data, frequency = 252, start = c(2014, 9, 17))
TBJ
data = read.csv("C:/Users/Administrator/Desktop/Time Series Forecasting/bitcoin_price", header = TRUE, sep = ",")
# Convert 'Date' column to Date type
data$Date = as.Date(data$Date, format = "%Y-%m-%d")
# Ensure it's sorted correctly
data <- data[order(data$Date), ]
#### Convert to time series data ####
TBJ = ts(data, frequency = 365.25, start = c(2014, 9, 17))
TBJ
BJ = BJsales
#### Convert to time series data ####
TBJ = ts(BJ, frequency = 12, start = c(2000, 1))
TBJ
library(xts)
data = read.csv("C:/Users/Administrator/Desktop/Time Series Forecasting/bitcoin_price", header = TRUE, sep = ",")
# Convert 'Date' column to Date type
data$Date = as.Date(data$Date, format = "%Y-%m-%d")
# Convert DataFrame to xts
data_xts <- xts(data[, -1], order.by = as.Date(data$Date))
# Resample to monthly frequency and compute mean
monthly_data_xts <- apply.monthly(data_xts, mean)
# Resample to monthly frequency and compute mean
monthly_data_xts <- apply.monthly(data_xts, colMeans)
# Print the result
head(monthly_data_xts)
#### Convert to time series data ####
TBJ = ts(data, frequency = 365.25, start = c(2014, 9, 17))
TBJ
#### Convert to time series data ####
TBJ = ts(monthly_data_xts, frequency = 365.25, start = c(2014, 9, 17))
TBJ
#### Convert to time series data ####
TBJ = ts(monthly_data_xts, frequency = 24, start = c(2014, 9, 17))
TBJ
# Print the result
head(monthly_data_xts)
#### Convert to time series data ####
TBJ = ts(monthly_data_xts, frequency = 24, start = c(2014, 9, 17))
TBJ
#### Convert to time series data ####
TBJ = ts(monthly_data_xts, frequency = 24, start = c(2014, 9, 30))
TBJ
#### Convert to time series data ####
TBJ = ts(BJ, frequency = 12, start = c(2000, 1))
TBJ
#### Convert to time series data ####
TBJ = ts(monthly_data_xts, frequency = 24, start = c(2014,12))
TBJ
#### Convert to time series data ####
TBJ = ts(monthly_data_xts, frequency = 12, start = c(2014,9))
TBJ
#### Exploring the Data ####
plot(TBJ)
#### ACF Plot ####
ggAcf(TBJ)
### Augmented Test ###
adf.test(TBJ)
### Phillips–Perron test ###
pp.test(TBJ)
# Select the 'Close' column as a univariate time series
TBJ_Close = TBJ[, "Close"]
### Augmented Test ###
adf.test(TBJ_Close)
### Phillips–Perron test ###
pp.test(TBJ_Close)
### KPSS Test ###
kpss.test(TBJ_Close)
#### Difference data
DTBJ = diff(TBJ_Close, differences=1)
DTBJ
#### Exploring the Data ####
plot(DTBJ)
### Augmented Test ###
adf.test(DTBJ)
### Phillips–Perron test ###
pp.test(DTBJ)
### KPSS Test ###
kpss.test(DTBJ)
ggAcf(DTBJ)
ggPacf(DTBJ)
Arima111= Arima(TBJ_Close,order=c(1,1,1),seasonal= c(0,0,0))
Arima111
Arima211= Arima(TBJ,order=c(2,1,1),seasonal= c(0,0,0))
Arima211= Arima(TBJ_Close,order=c(2,1,1),seasonal= c(0,0,0))
Arima211
# Coefficient Test for Arima Model ####
coeftest(Arima111)
residuals(Arima111)
plot(residuals(Arima111))
checkresiduals(Arima111)
ArchTest(residuals(Arima111))
forecast(Arima111, h=24)
plot(forecast(Arima111, h=24))
print(Arima111)
length(TBJ)
forecast(Arima111)
library(tseries)
library(lmtest)
library(FinTS)
library(forecast)
library(xts)
data = read.csv("C:/Users/Administrator/Desktop/Time Series Forecasting/bitcoin_price", header = TRUE, sep = ",")
data
#### Convert to time series data ####
TBJ = ts(data, frequency = 12, start = c(2014,9))
TBJ
# Convert 'Date' column to Date type
data$Date = as.Date(data$Date, format = "%Y-%m-%d")
#### Convert to time series data ####
TBJ = ts(data, frequency = 12, start = c(2014,9))
TBJ
#### Exploring the Data ####
plot(TBJ)
#### ACF Plot ####
ggAcf(TBJ)
# Select the 'Close' column as a univariate time series
TBJ_Close = TBJ[, "Close"]
### Augmented Test ###
adf.test(TBJ_Close)
### Phillips–Perron test ###
pp.test(TBJ_Close)
### KPSS Test ###
kpss.test(TBJ_Close)
#### Difference data
DTBJ = diff(TBJ_Close, differences=1)
DTBJ
#### Exploring the Data ####
plot(DTBJ)
### Augmented Test ###
adf.test(DTBJ)
### Phillips–Perron test ###
pp.test(DTBJ)
### KPSS Test ###
kpss.test(DTBJ)
ggAcf(DTBJ)
library(tseries)
library(lmtest)
library(FinTS)
library(forecast)
library(xts)
data = read.csv("C:/Users/Administrator/Desktop/Time Series Forecasting/bitcoin_price", header = TRUE, sep = ",")
data
# Convert 'Date' column to Date type
data$Date = as.Date(data$Date, format = "%Y-%m-%d")
# Convert DataFrame to xts
data_xts = xts(data[, -1], order.by = as.Date(data$Date))
# Resample to monthly frequency and compute mean
monthly_data_xts <- apply.monthly(data_xts, colMeans)
# Print the result
head(monthly_data_xts)
#### Convert to time series data ####
TBJ = ts(monthly_data_xts, frequency = 12, start = c(2014,9))
TBJ
#### Exploring the Data ####
plot(TBJ)
#### ACF Plot ####
ggAcf(TBJ)
### Augmented Test ###
adf.test(TBJ_Close)
### Phillips–Perron test ###
pp.test(TBJ_Close)
### KPSS Test ###
kpss.test(TBJ_Close)
ggAcf(DTBJ)
ggPacf(DTBJ)
library(tseries)
library(lmtest)
library(FinTS)
library(forecast)
library(xts)
data = read.csv("C:/Users/Administrator/Desktop/Time Series Forecasting/bitcoin_price", header = TRUE, sep = ",")
data
# Convert 'Date' column to Date type
data$Date = as.Date(data$Date, format = "%Y-%m-%d")
# Convert DataFrame to xts
data_xts = xts(data[, -1], order.by = as.Date(data$Date))
# Resample to monthly frequency and compute mean
monthly_data_xts = apply.monthly(data_xts, colMeans)
# Print the result
head(monthly_data_xts)
#### Convert to time series data ####
TBJ = ts(monthly_data_xts, frequency = 12, start = c(2014,9))
TBJ
#### Exploring the Data ####
plot(TBJ)
#### ACF Plot ####
ggAcf(TBJ)
# Select the 'Close' column as a univariate time series
TBJ_Close = TBJ[, "Close"]
### Augmented Test ###
adf.test(TBJ_Close)
### Phillips–Perron test ###
pp.test(TBJ_Close)
### KPSS Test ###
kpss.test(TBJ_Close)
#### Difference data
DTBJ = diff(TBJ_Close, differences=1)
DTBJ
#### Exploring the Data ####
plot(DTBJ)
### Augmented Test ###
adf.test(DTBJ)
### Phillips–Perron test ###
pp.test(DTBJ)
### KPSS Test ###
kpss.test(DTBJ)
ggAcf(DTBJ)
ggPacf(DTBJ)
Arima111= Arima(TBJ_Close,order=c(1,1,1),seasonal= c(0,0,0))
Arima111
Arima211= Arima(TBJ_Close,order=c(2,1,1),seasonal= c(0,0,0))
Arima211
library(tseries)
library(lmtest)
library(FinTS)
library(forecast)
library(xts)
data = read.csv("C:/Users/Administrator/Desktop/Time Series Forecasting/bitcoin_price", header = TRUE, sep = ",")
data
# Convert 'Date' column to Date type
data$Date = as.Date(data$Date, format = "%Y-%m-%d")
# Convert DataFrame to xts
data_xts = xts(data[, -1], order.by = as.Date(data$Date))
# Resample to monthly frequency and compute mean
monthly_data_xts = apply.monthly(data_xts, colMeans)
# Print the result
head(monthly_data_xts)
#### Convert to time series data ####
TBJ = ts(monthly_data_xts, frequency = 12, start = c(2014,9))
TBJ
#### Exploring the Data ####
plot(TBJ)
#### ACF Plot ####
ggAcf(TBJ)
# Select the 'Close' column as a univariate time series
TBJ_Close = TBJ[, "Close"]
### Augmented Test ###
adf.test(TBJ_Close)
### Phillips–Perron test ###
pp.test(TBJ_Close)
### KPSS Test ###
kpss.test(TBJ_Close)
#### Difference data
DTBJ = diff(TBJ_Close, differences=1)
DTBJ
#### Exploring the Data ####
plot(DTBJ)
### Augmented Test ###
adf.test(DTBJ)
### Phillips–Perron test ###
pp.test(DTBJ)
### KPSS Test ###
kpss.test(DTBJ)
ggAcf(DTBJ)
ggPacf(DTBJ)
Arima111= Arima(TBJ_Close,order=c(1,1,1),seasonal= c(0,0,0))
Arima111
Arima211= Arima(TBJ_Close,order=c(2,1,1),seasonal= c(0,0,0))
Arima211
# Coefficient Test for Arima Model ####
coeftest(Arima211)
residuals(Arima211)
plot(residuals(Arima211))
checkresiduals(Arima211)
ArchTest(residuals(Arima211))
forecast(Arima211, h=24)
plot(forecast(Arima211, h=24))
#Set working directory
setwd("C:/Users/Administrator/Desktop/Advance Data Analysis Exam/Covid_19_Topic_Modelling_Project")
# Load libraries
library(tidyverse)
library(tm)
library(textstem)
library(tidytext)
library(wordcloud)
library(ggplot2)
# Load dataset
tweets <- read_csv("covid19_tweets.csv")
tweets <- tweets %>% slice(1:2000)
# Check basic structure
glimpse(tweets)
# Preprocessing: Clean text
clean_text <- tweets %>%
select(text) %>%
mutate(text = tolower(text),
text = str_replace_all(text, "http\\S+|https\\S+", ""),  # Remove URLs
text = str_replace_all(text, "[^[:alpha:]\\s]", ""),     # Remove punctuation/numbers
text = removeWords(text, stopwords("en")),               # Remove stopwords
text = stripWhitespace(text))                            # Remove extra whitespace
# Tokenization
tokens <- clean_text %>%
unnest_tokens(word, text)
# Lemmatization
tokens <- tokens %>%
mutate(word = lemmatize_words(word))
# Remove empty words and low-information tokens
tokens <- tokens %>%
filter(nchar(word) > 2)
# Word frequency
word_freq <- tokens %>%
count(word, sort = TRUE)
# View top 20 words
print(head(word_freq, 20))
# Word cloud
set.seed(123)
wordcloud(words = word_freq$word,
freq = word_freq$n,
min.freq = 20,
max.words = 100,
random.order = FALSE,
colors = brewer.pal(8, "Dark2"))
# Load package
library(textmineR)
# Reconstruct cleaned documents
clean_docs <- tokens %>%
group_by(doc_id = row_number()) %>%
summarise(text = paste(word, collapse = " "))
# Create a document vector for textmineR
doc_vec <- clean_docs$text
names(doc_vec) <- paste0("doc", seq_along(doc_vec))  # Assign document names
# Create a DTM using textmineR's own function
dtm <- CreateDtm(doc_vec,
doc_names = names(doc_vec),
ngram_window = c(1, 1),   # Unigrams
stopword_vec = stopwords::stopwords("en"),
lower = TRUE,
remove_punctuation = TRUE,
remove_numbers = TRUE,
verbose = FALSE)
# Fit models for a range of topic numbers
k_list <- seq(2, 8, by = 2)
models <- lapply(k_list, function(k) {
FitLdaModel(dtm = dtm, k = k, iterations = 200, burnin = 80,
alpha = 0.1, beta = 0.05, optimize_alpha = TRUE,
calc_likelihood = TRUE, calc_coherence = TRUE, calc_r2 = TRUE)
})
# Extract coherence scores
coherence_scores <- sapply(models, function(m) mean(m$coherence))
# Plot to select best number of topics
plot(k_list, coherence_scores, type = "b", pch = 19,
xlab = "Number of Topics", ylab = "Mean Coherence Score",
main = "Topic Coherence vs. Number of Topics")
# Print topic numbers with their corresponding coherence scores
coherence_results <- data.frame(
Topics = k_list,
Coherence = coherence_scores
)
print(coherence_results)
# Set optimal number of topics (this number based is on the coherence plot)
optimal_k <- 6
# Fit final model
final_model <- FitLdaModel(dtm = dtm,
k = optimal_k,
iterations = 200, burnin = 80,
alpha = 0.1, beta = 0.05,
optimize_alpha = TRUE,
calc_likelihood = TRUE,
calc_coherence = TRUE,
calc_r2 = TRUE)
# Top terms per topic
top_terms <- GetTopTerms(final_model$phi, M = 10)
print(top_terms)
# Document-topic distributions
theta <- final_model$theta
head(theta)
# Assign dominant topic per document
doc_topics <- data.frame(doc_id = rownames(theta),
dominant_topic = apply(theta, 1, which.max))
head(doc_topics)
# Show example documents per topic
for (t in 1:optimal_k) {
cat(paste0("\n=== Topic ", t, " ===\n"))
sample_docs <- clean_docs$text[doc_topics$dominant_topic == t][1:2]  # Show 2 samples
print(sample_docs)
}
